{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc1495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0005\n",
    "PAITIENCE = 30\n",
    "\n",
    "IM_HEIGHT = 256\n",
    "IM_WIDTH = 256\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669a56c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_pos, train_neg  = 'data/tumor_classification/train/positive', 'data/tumor_classification/train/negative'\n",
    "valid_pos, valid_neg  = 'data/tumor_classification/valid/positive', 'data/tumor_classification/valid/negative'\n",
    "test_pos, test_neg    = 'data/tumor_classification/test/positive', 'data/tumor_classification/test/negative'\n",
    "\n",
    "def generate_dataframe(path_positive, path_negative):\n",
    "    df = pd.concat([\n",
    "            pd.DataFrame({\"image\": glob.glob(path_positive + \"/*.png\"), \"target\": 1}),\n",
    "            pd.DataFrame({\"image\": glob.glob(path_negative + \"/*.png\"), \"target\": 0}),\n",
    "        ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = generate_dataframe(train_pos, train_neg)\n",
    "valid_df = generate_dataframe(valid_pos, valid_neg)\n",
    "test_df  = generate_dataframe(test_pos, test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bcbbfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.augmentations.Resize(width=IM_HEIGHT, height=IM_WIDTH, p=1.0),\n",
    "    A.augmentations.HorizontalFlip(p=0.5),\n",
    "    A.augmentations.VerticalFlip(p=0.5),\n",
    "    A.augmentations.ShiftScaleRotate(p=0.5, shift_limit=0.05, scale_limit=0.1, rotate_limit=45),\n",
    "    A.ElasticTransform(p=0.5),\n",
    "    A.HueSaturationValue(p=0.5),\n",
    "    A.RandomBrightness(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.GaussNoise(p=0.3),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    # A.ImageCompression(p=0.3),\n",
    "    A.augmentations.Normalize(p=1.0),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "valid_transforms = A.Compose([\n",
    "    A.augmentations.Resize(width=IM_HEIGHT, height=IM_WIDTH, p=1.0),\n",
    "    A.augmentations.Normalize(p=1.0),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image  = cv2.imread(self.df.iloc[idx, 0])\n",
    "        target = self.df.iloc[idx, 1]\n",
    "\n",
    "        augmented = self.transforms(image=image)\n",
    "        image = augmented['image']  \n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return list(self.df.target.values)\n",
    "\n",
    "    \n",
    "class BalanceClassSampler(Sampler):\n",
    "    \"\"\"Abstraction over data sampler.\n",
    "    Allows you to create stratified sample on unbalanced classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, mode=\"downsampling\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (List[int]): list of class label\n",
    "                for each elem in the datasety\n",
    "            mode (str): Strategy to balance classes.\n",
    "                Must be one of [downsampling, upsampling]\n",
    "        \"\"\"\n",
    "        super().__init__(labels)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        samples_per_class = {\n",
    "            label: (labels == label).sum() for label in set(labels)\n",
    "        }\n",
    "\n",
    "        self.lbl2idx = {\n",
    "            label: np.arange(len(labels))[labels == label].tolist()\n",
    "            for label in set(labels)\n",
    "        }\n",
    "\n",
    "        if isinstance(mode, str):\n",
    "            assert mode in [\"downsampling\", \"upsampling\"]\n",
    "\n",
    "        if isinstance(mode, int) or mode == \"upsampling\":\n",
    "            samples_per_class = (\n",
    "                mode\n",
    "                if isinstance(mode, int)\n",
    "                else max(samples_per_class.values())\n",
    "            )\n",
    "        else:\n",
    "            samples_per_class = min(samples_per_class.values())\n",
    "\n",
    "        self.labels = labels\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.length = self.samples_per_class * len(set(labels))\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            indices of stratified sample\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for key in sorted(self.lbl2idx):\n",
    "            replace_ = self.samples_per_class > len(self.lbl2idx[key])\n",
    "            indices += np.random.choice(\n",
    "                self.lbl2idx[key], self.samples_per_class, replace=replace_\n",
    "            ).tolist()\n",
    "        assert len(indices) == self.length\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "             length of result sample\n",
    "        \"\"\"\n",
    "        return self.length\n",
    "    \n",
    "    \n",
    "train_dataset = TumorDataset(df=train_df, transforms=train_transforms)\n",
    "# train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=False, \n",
    "    sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode='downsampling'))\n",
    "\n",
    "valid_dataset = TumorDataset(df=valid_df, transforms=valid_transforms)\n",
    "# valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=False,\n",
    "    sampler=BalanceClassSampler(labels=valid_dataset.get_labels(), mode='downsampling'))\n",
    "\n",
    "test_dataset = TumorDataset(df=test_df, transforms=valid_transforms)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=1, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a88ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device       = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model        = timm.create_model('nf_resnet50', num_classes=2, pretrained=True).to(device)\n",
    "optimizer    = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler    = ReduceLROnPlateau(optimizer, 'min')\n",
    "class_weight = torch.tensor([1.0, 1.0]).to(device)\n",
    "criterion    = nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "#     if name[:4] == \"head\":\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ef5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, device=device):  \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct    = 0    \n",
    "    \n",
    "    for image, target in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        image  = image.to(device)\n",
    "        target = target.long().to(device)\n",
    "        \n",
    "        output = model(image).squeeze()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        pred     = torch.argmax(output, axis=1)\n",
    "        correct += (pred == target).sum()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss / len(iterator.dataset), correct / len(iterator.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, criterion, device=device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct    = 0 \n",
    "    \n",
    "    for image, target in iterator:\n",
    "        image  = image.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        output = model(image).squeeze()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        pred     = torch.argmax(output, axis=1)\n",
    "        correct += (pred == target).sum()\n",
    "            \n",
    "    return epoch_loss / len(iterator.dataset), correct / len(iterator.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, iterator, device=device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "    for image, target in iterator:\n",
    "        image  = image.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        output = model(image)\n",
    "\n",
    "        pred.append(output.to(\"cpu\").tolist()[0])\n",
    "        true.append(target.to(\"cpu\").tolist()[0])\n",
    "\n",
    "    return np.argmax(pred, axis=1), true\n",
    "\n",
    "\n",
    "def print_train_log(epoch_num, train_loss, valid_loss, train_acc, valid_acc):\n",
    "    print(f\"EPOCH: {epoch_num:04}\")\n",
    "    print(f\"Train loss: {round(train_loss, 4)}\\tTrain acc : {round(float(train_acc), 4)}\\tValid loss: {round(valid_loss, 4)}\\tValid acc : {round(float(valid_acc), 4)}\")\n",
    "    \n",
    "    \n",
    "def compute_test_metrics(true, pred):\n",
    "    confusion_mat = confusion_matrix(true, pred)\n",
    "    accuracy      = accuracy_score(true, pred)\n",
    "    precision     = precision_score(true, pred)\n",
    "    recall        = recall_score(true, pred)\n",
    "    f1            = f1_score(true, pred)\n",
    "    \n",
    "    return confusion_mat, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def print_test_log(accuracy, precision, recall, f1, epoch_num=None):\n",
    "    if epoch_num:\n",
    "        print(f\"EPOCH: {epoch_num:04} prediction results \")\n",
    "    else:\n",
    "        print(\"prediction results\")\n",
    "        \n",
    "    print(f\"confusion matrix\\n{confusion_mat}\")\n",
    "    print(f\"accuracy score  : {round(accuracy, 4)}\")\n",
    "    print(f\"precision score : {round(precision, 4)}\")\n",
    "    print(f\"recall score    : {round(recall, 4)}\")\n",
    "    print(f\"f1 score        : {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aee99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load trained model ... \n",
      "EPOCH: 0026\n",
      "Train loss: 0.0006\tTrain acc : 0.5909\tValid loss: 0.0005\tValid acc : 0.5968\n",
      "EPOCH: 0026 prediction results \n",
      "confusion matrix\n",
      "[[ 706   39]\n",
      " [  22 1631]]\n",
      "accuracy score  : 0.9746\n",
      "precision score : 0.9766\n",
      "recall score    : 0.9867\n",
      "f1 score        : 0.9816\n",
      "EPOCH: 0027\n",
      "Train loss: 0.0007\tTrain acc : 0.5861\tValid loss: 0.0005\tValid acc : 0.5945\n",
      "EPOCH: 0027 prediction results \n",
      "confusion matrix\n",
      "[[ 739    6]\n",
      " [ 121 1532]]\n",
      "accuracy score  : 0.947\n",
      "precision score : 0.9961\n",
      "recall score    : 0.9268\n",
      "f1 score        : 0.9602\n",
      "EPOCH: 0028\n",
      "Train loss: 0.0007\tTrain acc : 0.5878\tValid loss: 0.0007\tValid acc : 0.5861\n",
      "EPOCH: 0028 prediction results \n",
      "confusion matrix\n",
      "[[ 668   77]\n",
      " [  13 1640]]\n",
      "accuracy score  : 0.9625\n",
      "precision score : 0.9552\n",
      "recall score    : 0.9921\n",
      "f1 score        : 0.9733\n",
      "EPOCH: 0029\n",
      "Train loss: 0.0006\tTrain acc : 0.5915\tValid loss: 0.0004\tValid acc : 0.6001\n",
      "EPOCH: 0029 prediction results \n",
      "confusion matrix\n",
      "[[ 730   15]\n",
      " [  63 1590]]\n",
      "accuracy score  : 0.9675\n",
      "precision score : 0.9907\n",
      "recall score    : 0.9619\n",
      "f1 score        : 0.9761\n",
      "EPOCH: 0030\n",
      "Train loss: 0.0006\tTrain acc : 0.5918\tValid loss: 0.0003\tValid acc : 0.6046\n",
      "EPOCH: 0030 prediction results \n",
      "confusion matrix\n",
      "[[ 733   12]\n",
      " [  56 1597]]\n",
      "accuracy score  : 0.9716\n",
      "precision score : 0.9925\n",
      "recall score    : 0.9661\n",
      "f1 score        : 0.9792\n",
      "EPOCH: 0031\n",
      "Train loss: 0.0005\tTrain acc : 0.5947\tValid loss: 0.0004\tValid acc : 0.6044\n",
      "EPOCH: 0031 prediction results \n",
      "confusion matrix\n",
      "[[ 720   25]\n",
      " [  31 1622]]\n",
      "accuracy score  : 0.9766\n",
      "precision score : 0.9848\n",
      "recall score    : 0.9812\n",
      "f1 score        : 0.983\n",
      "EPOCH: 0032\n",
      "Train loss: 0.0006\tTrain acc : 0.5938\tValid loss: 0.0004\tValid acc : 0.6005\n",
      "EPOCH: 0032 prediction results \n",
      "confusion matrix\n",
      "[[ 715   30]\n",
      " [  36 1617]]\n",
      "accuracy score  : 0.9725\n",
      "precision score : 0.9818\n",
      "recall score    : 0.9782\n",
      "f1 score        : 0.98\n",
      "EPOCH: 0033\n",
      "Train loss: 0.0007\tTrain acc : 0.5892\tValid loss: 0.0004\tValid acc : 0.6015\n",
      "EPOCH: 0033 prediction results \n",
      "confusion matrix\n",
      "[[ 721   24]\n",
      " [  51 1602]]\n",
      "accuracy score  : 0.9687\n",
      "precision score : 0.9852\n",
      "recall score    : 0.9691\n",
      "f1 score        : 0.9771\n",
      "EPOCH: 0034\n",
      "Train loss: 0.0006\tTrain acc : 0.594\tValid loss: 0.0004\tValid acc : 0.6023\n",
      "EPOCH: 0034 prediction results \n",
      "confusion matrix\n",
      "[[ 741    4]\n",
      " [  76 1577]]\n",
      "accuracy score  : 0.9666\n",
      "precision score : 0.9975\n",
      "recall score    : 0.954\n",
      "f1 score        : 0.9753\n",
      "EPOCH: 0035\n",
      "Train loss: 0.0005\tTrain acc : 0.5948\tValid loss: 0.0003\tValid acc : 0.6042\n",
      "EPOCH: 0035 prediction results \n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  54 1599]]\n",
      "accuracy score  : 0.9716\n",
      "precision score : 0.9913\n",
      "recall score    : 0.9673\n",
      "f1 score        : 0.9792\n",
      "EPOCH: 0036\n",
      "Train loss: 0.0006\tTrain acc : 0.5935\tValid loss: 0.0004\tValid acc : 0.6025\n",
      "EPOCH: 0036 prediction results \n",
      "confusion matrix\n",
      "[[ 730   15]\n",
      " [  53 1600]]\n",
      "accuracy score  : 0.9716\n",
      "precision score : 0.9907\n",
      "recall score    : 0.9679\n",
      "f1 score        : 0.9792\n",
      "EPOCH: 0037\n",
      "Train loss: 0.0005\tTrain acc : 0.5949\tValid loss: 0.0005\tValid acc : 0.5993\n",
      "EPOCH: 0037 prediction results \n",
      "confusion matrix\n",
      "[[ 694   51]\n",
      " [  12 1641]]\n",
      "accuracy score  : 0.9737\n",
      "precision score : 0.9699\n",
      "recall score    : 0.9927\n",
      "f1 score        : 0.9812\n",
      "EPOCH: 0038\n",
      "Train loss: 0.0006\tTrain acc : 0.5949\tValid loss: 0.0004\tValid acc : 0.6032\n",
      "EPOCH: 0038 prediction results \n",
      "confusion matrix\n",
      "[[ 728   17]\n",
      " [  46 1607]]\n",
      "accuracy score  : 0.9737\n",
      "precision score : 0.9895\n",
      "recall score    : 0.9722\n",
      "f1 score        : 0.9808\n",
      "EPOCH: 0039\n",
      "Train loss: 0.0005\tTrain acc : 0.596\tValid loss: 0.0005\tValid acc : 0.6005\n",
      "EPOCH: 0039 prediction results \n",
      "confusion matrix\n",
      "[[ 739    6]\n",
      " [  78 1575]]\n",
      "accuracy score  : 0.965\n",
      "precision score : 0.9962\n",
      "recall score    : 0.9528\n",
      "f1 score        : 0.974\n",
      "EPOCH: 0040\n",
      "Train loss: 0.0005\tTrain acc : 0.5976\tValid loss: 0.0004\tValid acc : 0.6011\n",
      "EPOCH: 0040 prediction results \n",
      "confusion matrix\n",
      "[[ 741    4]\n",
      " [ 124 1529]]\n",
      "accuracy score  : 0.9466\n",
      "precision score : 0.9974\n",
      "recall score    : 0.925\n",
      "f1 score        : 0.9598\n",
      "EPOCH: 0041\n",
      "Train loss: 0.0005\tTrain acc : 0.5959\tValid loss: 0.0003\tValid acc : 0.6048\n",
      "EPOCH: 0041 prediction results \n",
      "confusion matrix\n",
      "[[ 723   22]\n",
      " [  40 1613]]\n",
      "accuracy score  : 0.9741\n",
      "precision score : 0.9865\n",
      "recall score    : 0.9758\n",
      "f1 score        : 0.9811\n",
      "EPOCH: 0042\n",
      "Train loss: 0.0004\tTrain acc : 0.6019\tValid loss: 0.0003\tValid acc : 0.6093\n",
      "EPOCH: 0042 prediction results \n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  46 1607]]\n",
      "accuracy score  : 0.975\n",
      "precision score : 0.9914\n",
      "recall score    : 0.9722\n",
      "f1 score        : 0.9817\n",
      "EPOCH: 0043\n",
      "Train loss: 0.0004\tTrain acc : 0.6036\tValid loss: 0.0003\tValid acc : 0.6087\n",
      "EPOCH: 0043 prediction results \n",
      "confusion matrix\n",
      "[[ 722   23]\n",
      " [  25 1628]]\n",
      "accuracy score  : 0.98\n",
      "precision score : 0.9861\n",
      "recall score    : 0.9849\n",
      "f1 score        : 0.9855\n",
      "EPOCH: 0044\n",
      "Train loss: 0.0004\tTrain acc : 0.6033\tValid loss: 0.0003\tValid acc : 0.6089\n",
      "EPOCH: 0044 prediction results \n",
      "confusion matrix\n",
      "[[ 738    7]\n",
      " [  45 1608]]\n",
      "accuracy score  : 0.9783\n",
      "precision score : 0.9957\n",
      "recall score    : 0.9728\n",
      "f1 score        : 0.9841\n",
      "EPOCH: 0045\n",
      "Train loss: 0.0003\tTrain acc : 0.606\tValid loss: 0.0003\tValid acc : 0.6079\n",
      "EPOCH: 0045 prediction results \n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  35 1618]]\n",
      "accuracy score  : 0.9796\n",
      "precision score : 0.9914\n",
      "recall score    : 0.9788\n",
      "f1 score        : 0.9851\n",
      "EPOCH: 0046\n",
      "Train loss: 0.0004\tTrain acc : 0.6045\tValid loss: 0.0003\tValid acc : 0.6097\n",
      "EPOCH: 0046 prediction results \n",
      "confusion matrix\n",
      "[[ 722   23]\n",
      " [  24 1629]]\n",
      "accuracy score  : 0.9804\n",
      "precision score : 0.9861\n",
      "recall score    : 0.9855\n",
      "f1 score        : 0.9858\n",
      "EPOCH: 0047\n",
      "Train loss: 0.0003\tTrain acc : 0.6063\tValid loss: 0.0003\tValid acc : 0.611\n",
      "EPOCH: 0047 prediction results \n",
      "confusion matrix\n",
      "[[ 736    9]\n",
      " [  42 1611]]\n",
      "accuracy score  : 0.9787\n",
      "precision score : 0.9944\n",
      "recall score    : 0.9746\n",
      "f1 score        : 0.9844\n",
      "EPOCH: 0048\n",
      "Train loss: 0.0003\tTrain acc : 0.6061\tValid loss: 0.0003\tValid acc : 0.6077\n",
      "EPOCH: 0048 prediction results \n",
      "confusion matrix\n",
      "[[ 725   20]\n",
      " [  25 1628]]\n",
      "accuracy score  : 0.9812\n",
      "precision score : 0.9879\n",
      "recall score    : 0.9849\n",
      "f1 score        : 0.9864\n",
      "EPOCH: 0049\n",
      "Train loss: 0.0003\tTrain acc : 0.6058\tValid loss: 0.0003\tValid acc : 0.6087\n",
      "EPOCH: 0049 prediction results \n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  30 1623]]\n",
      "accuracy score  : 0.9817\n",
      "precision score : 0.9914\n",
      "recall score    : 0.9819\n",
      "f1 score        : 0.9866\n",
      "EPOCH: 0050\n",
      "Train loss: 0.0003\tTrain acc : 0.6078\tValid loss: 0.0002\tValid acc : 0.6118\n",
      "EPOCH: 0050 prediction results \n",
      "confusion matrix\n",
      "[[ 729   16]\n",
      " [  25 1628]]\n",
      "accuracy score  : 0.9829\n",
      "precision score : 0.9903\n",
      "recall score    : 0.9849\n",
      "f1 score        : 0.9876\n",
      "EPOCH: 0051\n",
      "Train loss: 0.0003\tTrain acc : 0.6062\tValid loss: 0.0003\tValid acc : 0.6112\n",
      "EPOCH: 0051 prediction results \n",
      "confusion matrix\n",
      "[[ 732   13]\n",
      " [  36 1617]]\n",
      "accuracy score  : 0.9796\n",
      "precision score : 0.992\n",
      "recall score    : 0.9782\n",
      "f1 score        : 0.9851\n",
      "EPOCH: 0052\n",
      "Train loss: 0.0003\tTrain acc : 0.6058\tValid loss: 0.0003\tValid acc : 0.6081\n",
      "EPOCH: 0052 prediction results \n",
      "confusion matrix\n",
      "[[ 738    7]\n",
      " [  47 1606]]\n",
      "accuracy score  : 0.9775\n",
      "precision score : 0.9957\n",
      "recall score    : 0.9716\n",
      "f1 score        : 0.9835\n",
      "EPOCH: 0053\n",
      "Train loss: 0.0003\tTrain acc : 0.606\tValid loss: 0.0003\tValid acc : 0.6093\n",
      "EPOCH: 0053 prediction results \n",
      "confusion matrix\n",
      "[[ 723   22]\n",
      " [  19 1634]]\n",
      "accuracy score  : 0.9829\n",
      "precision score : 0.9867\n",
      "recall score    : 0.9885\n",
      "f1 score        : 0.9876\n",
      "EPOCH: 0054\n",
      "Train loss: 0.0003\tTrain acc : 0.6067\tValid loss: 0.0003\tValid acc : 0.6089\n",
      "EPOCH: 0054 prediction results \n",
      "confusion matrix\n",
      "[[ 721   24]\n",
      " [  16 1637]]\n",
      "accuracy score  : 0.9833\n",
      "precision score : 0.9856\n",
      "recall score    : 0.9903\n",
      "f1 score        : 0.9879\n",
      "EPOCH: 0055\n",
      "Train loss: 0.0003\tTrain acc : 0.6065\tValid loss: 0.0003\tValid acc : 0.6097\n",
      "EPOCH: 0055 prediction results \n",
      "confusion matrix\n",
      "[[ 720   25]\n",
      " [  21 1632]]\n",
      "accuracy score  : 0.9808\n",
      "precision score : 0.9849\n",
      "recall score    : 0.9873\n",
      "f1 score        : 0.9861\n",
      "EPOCH: 0056\n",
      "Train loss: 0.0003\tTrain acc : 0.6082\tValid loss: 0.0003\tValid acc : 0.6106\n",
      "EPOCH: 0056 prediction results \n",
      "confusion matrix\n",
      "[[ 736    9]\n",
      " [  39 1614]]\n",
      "accuracy score  : 0.98\n",
      "precision score : 0.9945\n",
      "recall score    : 0.9764\n",
      "f1 score        : 0.9853\n",
      "EPOCH: 0057\n",
      "Train loss: 0.0003\tTrain acc : 0.608\tValid loss: 0.0003\tValid acc : 0.6102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0057 prediction results \n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  27 1626]]\n",
      "accuracy score  : 0.9829\n",
      "precision score : 0.9915\n",
      "recall score    : 0.9837\n",
      "f1 score        : 0.9875\n",
      "EPOCH: 0058\n",
      "Train loss: 0.0003\tTrain acc : 0.6088\tValid loss: 0.0003\tValid acc : 0.6093\n",
      "EPOCH: 0058 prediction results \n",
      "confusion matrix\n",
      "[[ 730   15]\n",
      " [  37 1616]]\n",
      "accuracy score  : 0.9783\n",
      "precision score : 0.9908\n",
      "recall score    : 0.9776\n",
      "f1 score        : 0.9842\n",
      "EPOCH: 0059\n",
      "Train loss: 0.0003\tTrain acc : 0.6082\tValid loss: 0.0002\tValid acc : 0.611\n",
      "EPOCH: 0059 prediction results \n",
      "confusion matrix\n",
      "[[ 733   12]\n",
      " [  27 1626]]\n",
      "accuracy score  : 0.9837\n",
      "precision score : 0.9927\n",
      "recall score    : 0.9837\n",
      "f1 score        : 0.9881\n",
      "EPOCH: 0060\n",
      "Train loss: 0.0003\tTrain acc : 0.6076\tValid loss: 0.0002\tValid acc : 0.6104\n",
      "EPOCH: 0060 prediction results \n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  27 1626]]\n",
      "accuracy score  : 0.9829\n",
      "precision score : 0.9915\n",
      "recall score    : 0.9837\n",
      "f1 score        : 0.9875\n",
      "EPOCH: 0061\n",
      "Train loss: 0.0003\tTrain acc : 0.6067\tValid loss: 0.0002\tValid acc : 0.6112\n",
      "EPOCH: 0061 prediction results \n",
      "confusion matrix\n",
      "[[ 732   13]\n",
      " [  27 1626]]\n",
      "accuracy score  : 0.9833\n",
      "precision score : 0.9921\n",
      "recall score    : 0.9837\n",
      "f1 score        : 0.9878\n",
      "EPOCH: 0062\n",
      "Train loss: 0.0003\tTrain acc : 0.6085\tValid loss: 0.0003\tValid acc : 0.6097\n",
      "EPOCH: 0062 prediction results \n",
      "confusion matrix\n",
      "[[ 722   23]\n",
      " [  23 1630]]\n",
      "accuracy score  : 0.9808\n",
      "precision score : 0.9861\n",
      "recall score    : 0.9861\n",
      "f1 score        : 0.9861\n",
      "EPOCH: 0063\n",
      "Train loss: 0.0003\tTrain acc : 0.6082\tValid loss: 0.0003\tValid acc : 0.6093\n",
      "EPOCH: 0063 prediction results \n",
      "confusion matrix\n",
      "[[ 735   10]\n",
      " [  30 1623]]\n",
      "accuracy score  : 0.9833\n",
      "precision score : 0.9939\n",
      "recall score    : 0.9819\n",
      "f1 score        : 0.9878\n",
      "EPOCH: 0064\n",
      "Train loss: 0.0003\tTrain acc : 0.6091\tValid loss: 0.0003\tValid acc : 0.6099\n",
      "EPOCH: 0064 prediction results \n",
      "confusion matrix\n",
      "[[ 728   17]\n",
      " [  28 1625]]\n",
      "accuracy score  : 0.9812\n",
      "precision score : 0.9896\n",
      "recall score    : 0.9831\n",
      "f1 score        : 0.9863\n",
      "EPOCH: 0065\n",
      "Train loss: 0.0003\tTrain acc : 0.6085\tValid loss: 0.0003\tValid acc : 0.6087\n",
      "EPOCH: 0065 prediction results \n",
      "confusion matrix\n",
      "[[ 729   16]\n",
      " [  29 1624]]\n",
      "accuracy score  : 0.9812\n",
      "precision score : 0.9902\n",
      "recall score    : 0.9825\n",
      "f1 score        : 0.9863\n",
      "EPOCH: 0066\n",
      "Train loss: 0.0003\tTrain acc : 0.6088\tValid loss: 0.0002\tValid acc : 0.6128\n",
      "EPOCH: 0066 prediction results \n",
      "confusion matrix\n",
      "[[ 724   21]\n",
      " [  21 1632]]\n",
      "accuracy score  : 0.9825\n",
      "precision score : 0.9873\n",
      "recall score    : 0.9873\n",
      "f1 score        : 0.9873\n",
      "EPOCH: 0067\n",
      "Train loss: 0.0003\tTrain acc : 0.6093\tValid loss: 0.0003\tValid acc : 0.6091\n",
      "EPOCH: 0067 prediction results \n",
      "confusion matrix\n",
      "[[ 724   21]\n",
      " [  22 1631]]\n",
      "accuracy score  : 0.9821\n",
      "precision score : 0.9873\n",
      "recall score    : 0.9867\n",
      "f1 score        : 0.987\n",
      "EPOCH: 0068\n",
      "Train loss: 0.0003\tTrain acc : 0.6083\tValid loss: 0.0003\tValid acc : 0.6106\n",
      "EPOCH: 0068 prediction results \n",
      "confusion matrix\n",
      "[[ 726   19]\n",
      " [  22 1631]]\n",
      "accuracy score  : 0.9829\n",
      "precision score : 0.9885\n",
      "recall score    : 0.9867\n",
      "f1 score        : 0.9876\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if len(glob.glob(\"output/tumor_classification/nfnet/*.txt\")) != 0:\n",
    "    print(\"load trained model ... \")\n",
    "    start_epoch = len(glob.glob(\"output/tumor_classification/nfnet/*.txt\")) - 1 \n",
    "    model.load_state_dict(torch.load('weights/tumor_classification/best_nfnet.pt'))\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "for epoch_num in range(start_epoch, N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, criterion, optimizer, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    print_train_log(epoch_num, train_loss, valid_loss, train_acc, valid_acc)\n",
    "    with open(\"output/tumor_classification/nfnet/log.txt\", \"a\") as f:\n",
    "        f.write(\"epoch: {0:04d} train loss: {1:.4f}, valid loss: {2:.4f}, train Acc: {3:.4f}, valid Acc: {4:.4f}\\n\".format(epoch_num, train_loss, valid_loss, train_acc, valid_acc))\n",
    "\n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'weights/tumor_classification/best_nfnet.pt')\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        model.load_state_dict(torch.load('weights/tumor_classification/best_nfnet.pt'))\n",
    "        break\n",
    "        \n",
    "    if epoch_num % 1 == 0:\n",
    "        \n",
    "        pred, true = predict(model, test_iterator)\n",
    "        confusion_mat, accuracy, precision, recall, f1 = compute_test_metrics(true, pred)\n",
    "        \n",
    "        print_test_log(accuracy, precision, recall, f1, epoch_num=epoch_num)\n",
    "        with open(\"output/tumor_classification/nfnet/epoch_{0:04d}_eval_metrics.txt\".format(epoch_num), \"a\") as f:\n",
    "            f.write(\"accuracy score: {0:.4f}, precision score: {1:.4f}, recall score: {2:.4f}, f1 score: {3:.4f}\\n\".format(accuracy, precision, recall, f1))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f4c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction results\n",
      "confusion matrix\n",
      "[[ 731   14]\n",
      " [  32 1621]]\n",
      "accuracy score  : 0.9808\n",
      "precision score : 0.9914\n",
      "recall score    : 0.9806\n",
      "f1 score        : 0.986\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights/tumor_classification/best_nfnet.pt'))\n",
    "pred, true = predict(model, test_iterator)\n",
    "confusion_mat, accuracy, precision, recall, f1 = compute_test_metrics(true, pred)\n",
    "\n",
    "print_test_log(accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e688a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
