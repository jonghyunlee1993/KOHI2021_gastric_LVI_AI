{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e28b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0005\n",
    "PAITIENCE = 30\n",
    "\n",
    "IM_HEIGHT = 256\n",
    "IM_WIDTH = 256\n",
    "\n",
    "model_name = \"resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8052d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (0, 4)\n",
      "valid_df: (0, 4)\n",
      "test_df: (0, 4)\n"
     ]
    }
   ],
   "source": [
    "def generate_patch_df(flist, label):\n",
    "    df = pd.DataFrame({\"fpath\": flist})\n",
    "    df['slide_id'] = df['fpath'].map(lambda x: x.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0])\n",
    "    df['patient_id'] = df['slide_id'].map(lambda x: x.split(\"-\")[0])\n",
    "    df['target'] = label\n",
    "\n",
    "    df = df.loc[:, [\"patient_id\", \"slide_id\", \"fpath\", \"target\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test_split(positive_df, negative_df, sampling_level=2, sampling_rate=0.2):\n",
    "    # sampling_level {0: \"patient_id\", 1: \"slide_id\", 2: \"patch\"}\n",
    "    if sampling_level == 0:\n",
    "        column_name = \"patient_id\"\n",
    "    elif sampling_level == 1:\n",
    "        column_name = \"slide_id\"\n",
    "    elif sampling_level == 2:\n",
    "        column_name = \"fpath\"\n",
    "    else:\n",
    "        print(\"Set sampling level in [0, 1, 2]\")\n",
    "        raise\n",
    "    \n",
    "    N = len(pd.unique(positive_df[column_name]))\n",
    "    \n",
    "    test_index = np.random.choice(pd.unique(positive_df[column_name]), round(N * sampling_rate), replace=False)\n",
    "    \n",
    "    train_positive = positive_df[~positive_df[column_name].isin(test_index)]\n",
    "    test_positive = positive_df[positive_df[column_name].isin(test_index)]\n",
    "\n",
    "    train_negative = negative_df[~negative_df[column_name].isin(test_index)]\n",
    "    test_negative = negative_df[negative_df[column_name].isin(test_index)]\n",
    "    \n",
    "    train_df = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "    test_df = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "    \n",
    "    train_df, valid_df = train_valid_split(train_df, column_name, sampling_rate)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "def train_valid_split(train_df, column_name, sampling_rate):\n",
    "    N = len(pd.unique(train_df[column_name]))\n",
    "    valid_index = np.random.choice(pd.unique(train_df[column_name]), round(N * sampling_rate), replace=False)\n",
    "        \n",
    "    valid_df = train_df[train_df[column_name].isin(valid_index)]\n",
    "    train_df = train_df[~train_df[column_name].isin(valid_index)]\n",
    "    \n",
    "    return train_df.reset_index(drop=True), valid_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "positive_flist = glob.glob(\"../data/LVI_dataset/patch_image_size-400_overlap-100/positive/*.png\")\n",
    "negative_flist = glob.glob(\"../data/LVI_dataset/patch_image_size-400_overlap-100/negative/*.png\")\n",
    "\n",
    "positive_df = generate_patch_df(positive_flist, 1)\n",
    "negative_df = generate_patch_df(negative_flist, 0)\n",
    "\n",
    "train_df, valid_df, test_df = train_test_split(positive_df, negative_df, sampling_level=2, sampling_rate=0.2)\n",
    "print(f\"train_df: {train_df.shape}\\nvalid_df: {valid_df.shape}\\ntest_df: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a64ff78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26640/1045467385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLVIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8, shuffle=False, \n\u001b[0;32m--> 130\u001b[0;31m     sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode='downsampling'))\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLVIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26640/1045467385.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, labels, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0msamples_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_per_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "train_transforms = A.Compose([ \n",
    "    A.RandomCrop(width=IM_WIDTH, height=IM_HEIGHT, p=1.0),\n",
    "    \n",
    "    A.OneOf([\n",
    "        A.Transpose(),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip()\n",
    "    ], p=0.5),\n",
    "\n",
    "    A.OneOf([\n",
    "       A.ShiftScaleRotate(),\n",
    "       A.ElasticTransform(),\n",
    "       A.RandomScale()\n",
    "    ], p=0.5),\n",
    "\n",
    "    A.OneOf([\n",
    "       A.Blur(),\n",
    "       A.GaussianBlur(),\n",
    "       A.GaussNoise(),\n",
    "       A.MedianBlur()\n",
    "    ], p=0.5),\n",
    "\n",
    "    A.OneOf([\n",
    "       A.ChannelShuffle(),\n",
    "       A.ColorJitter(),\n",
    "       A.HueSaturationValue(),\n",
    "       A.RandomBrightnessContrast()\n",
    "    ], p=0.5),\n",
    "\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "valid_transforms = A.Compose([ \n",
    "    A.Resize(width=IM_WIDTH, height=IM_HEIGHT, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "class LVIDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image  = cv2.imread(self.df.iloc[idx, 0])\n",
    "        target = self.df.iloc[idx, 1]\n",
    "\n",
    "        augmented = self.transforms(image=image)\n",
    "        image = augmented['image']  \n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return list(self.df.target.values)\n",
    "\n",
    "\n",
    "class BalanceClassSampler(Sampler):\n",
    "    \"\"\"Abstraction over data sampler.\n",
    "    Allows you to create stratified sample on unbalanced classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, mode=\"downsampling\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (List[int]): list of class label\n",
    "                for each elem in the datasety\n",
    "            mode (str): Strategy to balance classes.\n",
    "                Must be one of [downsampling, upsampling]\n",
    "        \"\"\"\n",
    "        super().__init__(labels)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        samples_per_class = {\n",
    "            label: (labels == label).sum() for label in set(labels)\n",
    "        }\n",
    "\n",
    "        self.lbl2idx = {\n",
    "            label: np.arange(len(labels))[labels == label].tolist()\n",
    "            for label in set(labels)\n",
    "        }\n",
    "\n",
    "        if isinstance(mode, str):\n",
    "            assert mode in [\"downsampling\", \"upsampling\"]\n",
    "\n",
    "        if isinstance(mode, int) or mode == \"upsampling\":\n",
    "            samples_per_class = (\n",
    "                mode\n",
    "                if isinstance(mode, int)\n",
    "                else max(samples_per_class.values())\n",
    "            )\n",
    "        else:\n",
    "            samples_per_class = min(samples_per_class.values())\n",
    "\n",
    "        self.labels = labels\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.length = self.samples_per_class * len(set(labels))\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            indices of stratified sample\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for key in sorted(self.lbl2idx):\n",
    "            replace_ = self.samples_per_class > len(self.lbl2idx[key])\n",
    "            indices += np.random.choice(\n",
    "                self.lbl2idx[key], self.samples_per_class, replace=replace_\n",
    "            ).tolist()\n",
    "        assert len(indices) == self.length\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "             length of result sample\n",
    "        \"\"\"\n",
    "        return self.length\n",
    "\n",
    "    \n",
    "train_dataset = LVIDataset(df=train_df, transforms=train_transforms)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8, shuffle=False, \n",
    "    sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode='downsampling'))\n",
    "\n",
    "valid_dataset = LVIDataset(df=valid_df, transforms=valid_transforms)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=8, shuffle=False,\n",
    "    sampler=BalanceClassSampler(labels=valid_dataset.get_labels(), mode='downsampling'))\n",
    "\n",
    "test_dataset = LVIDataset(df=test_df, transforms=valid_transforms)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device       = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "model        = timm.create_model(model_name, num_classes=2, pretrained=True).to(device)\n",
    "optimizer    = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler    = ReduceLROnPlateau(optimizer, 'min')\n",
    "class_weight = torch.tensor([1.1, 1.0]).to(device)\n",
    "criterion    = nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "#     if name[:4] == \"head\":\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f795ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, device=device):  \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct    = 0    \n",
    "    \n",
    "    for image, target in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        image  = image.to(device)\n",
    "        target = target.long().to(device)\n",
    "        \n",
    "        output = model(image).squeeze()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        pred     = torch.argmax(output, axis=1)\n",
    "        correct += (pred == target).sum()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss / len(iterator.dataset), correct / len(iterator.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, criterion, device=device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct    = 0 \n",
    "    \n",
    "    for image, target in iterator:\n",
    "        image  = image.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        output = model(image).squeeze()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        pred     = torch.argmax(output, axis=1)\n",
    "        correct += (pred == target).sum()\n",
    "            \n",
    "    return epoch_loss / len(iterator.dataset), correct / len(iterator.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, iterator, device=device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "    for image, target in iterator:\n",
    "        image  = image.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        output = model(image)\n",
    "\n",
    "        pred.append(output.to(\"cpu\").tolist()[0])\n",
    "        true.append(target.to(\"cpu\").tolist()[0])\n",
    "\n",
    "    return np.argmax(pred, axis=1), true\n",
    "\n",
    "\n",
    "def print_train_log(epoch_num, train_loss, valid_loss, train_acc, valid_acc):\n",
    "    print(f\"EPOCH: {epoch_num:04}\")\n",
    "    print(f\"Train loss: {round(train_loss, 4)}\\tTrain acc : {round(float(train_acc), 4)}\\tValid loss: {round(valid_loss, 4)}\\tValid acc : {round(float(valid_acc), 4)}\")\n",
    "    \n",
    "    \n",
    "def compute_test_metrics(true, pred):\n",
    "    confusion_mat = confusion_matrix(true, pred)\n",
    "    accuracy      = accuracy_score(true, pred)\n",
    "    precision     = precision_score(true, pred)\n",
    "    recall        = recall_score(true, pred)\n",
    "    f1            = f1_score(true, pred)\n",
    "    \n",
    "    return confusion_mat, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def print_test_log(epoch_num, accuracy, precision, recall, f1):\n",
    "    print(f\"EPOCH: {epoch_num:04} prediction results \")\n",
    "    print(f\"confusion matrix\\n{confusion_mat}\")\n",
    "    print(f\"accuracy score  : {round(accuracy, 4)}\")\n",
    "    print(f\"precision score : {round(precision, 4)}\")\n",
    "    print(f\"recall score    : {round(recall, 4)}\")\n",
    "    print(f\"f1 score        : {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a8c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if len(glob.glob()) != 0:\n",
    "    print(\"load trained model ... \")\n",
    "    start_epoch = len(glob.glob(os.path.join(os.path.join(\"output\", model_name, \"*.txt\")))) - 1 \n",
    "    model.load_state_dict(torch.load(\"weights/best_\" + model_name + \".pt\"))\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "for epoch_num in range(start_epoch, N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, criterion, optimizer, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    print_train_log(epoch_num, train_loss, valid_loss, train_acc, valid_acc)\n",
    "    with open(\"output/resnet50/log.txt\", \"a\") as f:\n",
    "        f.write(\"epoch: {0:04d} train loss: {1:.4f}, valid loss: {2:.4f}, train Acc: {3:.4f}, valid Acc: {4:.4f}\\n\".format(epoch_num, train_loss, valid_loss, train_acc, valid_acc))\n",
    "\n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"weights/best_\" + model_name + \".pt\")\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        model.load_state_dict(torch.load(\"weights/best_\" + model_name + \".pt\"))\n",
    "        break\n",
    "        \n",
    "    if epoch_num % 1 == 0:\n",
    "        \n",
    "        pred, true = predict(model, test_iterator)\n",
    "        confusion_mat, accuracy, precision, recall, f1 = compute_test_metrics(true, pred)\n",
    "        \n",
    "        print_test_log(epoch_num, accuracy, precision, recall, f1)\n",
    "        with open(os.path.join(\"output\", model_name, f\"epoch_{epoch_num:04d}_eval_metrics.txt\"), \"a\") as f:\n",
    "            f.write(\"accuracy score: {0:.4f}, precision score: {1:.4f}, recall score: {2:.4f}, f1 score: {3:.4f}\\n\".format(accuracy, precision, recall, f1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7b2e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0133 prediction results \n",
      "confusion matrix\n",
      "[[ 736    9]\n",
      " [  17 1636]]\n",
      "accuracy score  : 0.9892\n",
      "precision score : 0.9945\n",
      "recall score    : 0.9897\n",
      "f1 score        : 0.9921\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights/tumor_classification/best_resnet50.pt'))\n",
    "\n",
    "pred, true = predict(model, test_iterator)\n",
    "confusion_mat, accuracy, precision, recall, f1 = compute_test_metrics(true, pred)\n",
    "        \n",
    "print_test_log(epoch_num, accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae8624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
